{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa2897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "from os.path import exists\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "95c16ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPortfolio:\n",
    "    \"\"\"\n",
    "    Python ETL script for Friktion user portfolio data. \n",
    "    \n",
    "    Currently supported Instruction Names: \n",
    "        - Deposit\n",
    "        - CancelPendingDeposit\n",
    "        - Withdrawal\n",
    "        - CancelPendingWithdrawal\n",
    "        - ClaimPendingWithdrawal\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 date_start, \n",
    "                 date_end, \n",
    "                 ix_fname='friktion_ix.csv', \n",
    "                 deposit_fname='friktion_deposit.csv', \n",
    "                 deposit_cxl_fname='friktion_deposit_cxl.csv', \n",
    "                 withdraw_fname='friktion_withdraw.csv', \n",
    "                 withdraw_cxl_fname='friktion_withdraw_cancel.csv',\n",
    "                 withdraw_claim_fname='friktion_claim_withdrawal.csv',\n",
    "                 batch_size_days=14, \n",
    "                 batch_size_xfers=75\n",
    "            ):\n",
    "        \"\"\"\n",
    "        :ix_fname:              output csv for instructions\n",
    "        :deposit_fname:         output csv for deposits\n",
    "        :deposit_cxl_fname:     output csv for deposit cancels\n",
    "        :withdraw_fname:        output csv for withdrawals\n",
    "        :withdraw_cxl_fname:    output csv for withdrawal cancels\n",
    "        :withdraw_claim_fname:    output csv for claiming pending withdrawal\n",
    "        :batch_size_days:       batch size in days for query to keep query < 10k rows. Use bigger steps for larger data.\n",
    "        :batch_size_transfers:  batch size transactions for query to keep query < 8kb \n",
    "\n",
    "        \"\"\"\n",
    "        self.volt_program = \"VoLT1mJz1sbnxwq5Fv2SXjdVDgPXrb9tJyC8WpMDkSp\"\n",
    "        self.date_start = date_start\n",
    "        self.date_end = date_end\n",
    "        self.ix_fname = ix_fname\n",
    "        self.deposit_fname = deposit_fname\n",
    "        self.deposit_cxl_fname = deposit_cxl_fname\n",
    "        self.withdraw_fname = withdraw_fname\n",
    "        self.withdraw_cxl_fname = withdraw_cxl_fname\n",
    "        self.withdraw_claim_fname = withdraw_claim_fname\n",
    "        self.batch_size_days = batch_size_days\n",
    "        self.batch_size_xfers = batch_size_xfers\n",
    "        self.df_ix = []\n",
    "        self.friktion_metadata = self.get_friktion_snapshot()\n",
    "\n",
    "\n",
    "    ########################################################################################################\n",
    "    ####################################          Queries             ######################################\n",
    "    ########################################################################################################\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def ix_query(self):\n",
    "        return \"\"\"\n",
    "            query MyQuery {\n",
    "              solana {\n",
    "                instructions(\n",
    "                  time: {between: [\"%s\", \"%s\"]}\n",
    "                  success: {is: true}\n",
    "                  programId: {is: \"VoLT1mJz1sbnxwq5Fv2SXjdVDgPXrb9tJyC8WpMDkSp\"}\n",
    "                ) {\n",
    "                  block {\n",
    "                    timestamp {\n",
    "                      iso8601\n",
    "                    }\n",
    "                  }\n",
    "                  log {\n",
    "                    consumed\n",
    "                    logs\n",
    "                  }\n",
    "                  transaction {\n",
    "                    signature\n",
    "                    success\n",
    "                    feePayer\n",
    "                  }\n",
    "                  data {\n",
    "                    base58\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def xfer_query(self):\n",
    "        return \"\"\"\n",
    "            query MyQuery {\n",
    "              solana(network: solana) {\n",
    "                transfers(\n",
    "                  signature: {in: [%s]}\n",
    "                ) {\n",
    "                  instruction {\n",
    "                    action {\n",
    "                      name\n",
    "                      type\n",
    "                    }\n",
    "                    callPath\n",
    "                  }\n",
    "                  amount(success: {is: true})\n",
    "                  transaction {\n",
    "                    signer\n",
    "                    signature\n",
    "                  }\n",
    "                  block {\n",
    "                    timestamp {\n",
    "                      iso8601\n",
    "                    }\n",
    "                  }\n",
    "                  currency {\n",
    "                    name\n",
    "                    address\n",
    "                    symbol\n",
    "                    decimals\n",
    "                  }\n",
    "                  sender {\n",
    "                    address\n",
    "                    mintAccount\n",
    "                  }\n",
    "                  receiver {\n",
    "                    address\n",
    "                    mintAccount\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    ########################################################################################################\n",
    "    ################################          Helper Functions             #################################\n",
    "    ########################################################################################################\n",
    "    \n",
    "    \n",
    "    # TODO: Add retry logic to this in case of hangups. \n",
    "    @staticmethod\n",
    "    def run_query(query):  # A simple function to use requests.post to make the API call.\n",
    "        headers = {'X-API-KEY': 'BQYCaXaMZlqZrPCSQVsiJrKtxKRVcSe4'}\n",
    "        request = requests.post('https://graphql.bitquery.io/', json={'query': query}, headers=headers)\n",
    "        if request.status_code == 200:\n",
    "            return request.json()\n",
    "        else:\n",
    "            print(request.reason)\n",
    "            raise Exception('Query failed and return code is {}.{}'.format(request.status_code, query))\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_iterable(iterable, n=1):\n",
    "        \"\"\"\n",
    "        Takes in an iterable and returns an iterable of iterables with len==x\n",
    "        \"\"\"\n",
    "        idxs = []\n",
    "        l = len(iterable)\n",
    "        for idx in range(0, l, n):\n",
    "            idxs.append(iterable[idx:min(idx+n, l)])\n",
    "        return idxs\n",
    "        \n",
    "        \n",
    "    def format_txs_for_query(self, tx_signatures):\n",
    "        \"\"\"\n",
    "        Batches a list of transactions into a list of string formatted transactions for querying. \n",
    "        Each of these strings contain (n=self.batch_size_xfers) unique transaction IDs.\n",
    "        \"\"\"\n",
    "        batched_signatures = self.batch_iterable(tx_signatures, self.batch_size_xfers)\n",
    "        \n",
    "        def format_txs(x):\n",
    "            return str(x)[1:-1].replace(\"\\'\", \"\\\"\").replace(\"\\n\", \"\")\n",
    "        \n",
    "        tx_strs = list(map(format_txs, batched_signatures))\n",
    "\n",
    "        return tx_strs\n",
    "    \n",
    "    \n",
    "    def get_existing_df(self, fname):\n",
    "        # Create output file if doesn't exist\n",
    "        if fname and exists(fname):\n",
    "            return pd.read_csv(fname)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    @staticmethod\n",
    "    def instruction_match(instructionData):\n",
    "        if not instructionData or len(instructionData) < 8:\n",
    "            return False\n",
    "        \n",
    "        instructionDescriptor = instructionData[:8]\n",
    "        \n",
    "        if instructionDescriptor == \"PcB3tF1K\":\n",
    "            return \"Withdraw\"\n",
    "        elif instructionDescriptor == \"WuE7Hjns\":\n",
    "            return \"Deposit\"\n",
    "        elif instructionDescriptor == \"V8cW2nMq\":\n",
    "            return \"CancelPendingDeposit\"\n",
    "        elif instructionDescriptor == \"dxUbSCWk\":\n",
    "            return \"CancelPendingWithdrawal\"\n",
    "        elif instructionDescriptor == \"WcTWQsnk\":\n",
    "            return \"ClaimPendingWithdrawal\"\n",
    "        else:\n",
    "            return \"Unclassified\"\n",
    "        \n",
    "        \n",
    "    def get_friktion_snapshot(self):\n",
    "        \"\"\"\n",
    "        Load Friktion Metadata for Volt/Symbol Mapping to join to normal data\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.DataFrame(\n",
    "                dict(\n",
    "                    json.loads(\n",
    "                        requests.get(\"https://friktion-labs.github.io/mainnet-tvl-snapshots/friktionSnapshot.json\"\n",
    "                        ).content)\n",
    "                  )['allMainnetVolts']\n",
    "            )[[\"globalId\", \"vaultAuthority\", \"shareTokenMint\", \"depositTokenSymbol\", \"depositTokenCoingeckoId\"]]\n",
    "        except Exception as e:\n",
    "            print(datetime.now(), \"Snapshot Data Invalid\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "            \n",
    "    ########################################################################################################\n",
    "    ################################          Data Retrieval             ###################################\n",
    "    ########################################################################################################\n",
    "    \n",
    "    \n",
    "    def get_ix(self, date_start, date_end):\n",
    "        \"\"\"\n",
    "        Runs graphql instruction query for one date range. \n",
    "        \"\"\"\n",
    "        print(date_start, date_end)\n",
    "        query = self.ix_query % (date_start, date_end)\n",
    "        print(datetime.now(), \"retrieving instructions for {} to {}\".format(date_start, date_end))\n",
    "        result = self.run_query(query)\n",
    "        \n",
    "        # convert GraphQL json to pandas dataframe\n",
    "        df = pd.json_normalize(result['data']['solana']['instructions'])\n",
    "        print(datetime.now(), df.shape[0], \"instructions retrieved\")\n",
    "        \n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                \"block.timestamp.iso8601\": \"timestamp\", \n",
    "                \"log.consumed\": \"computeUnits\", \n",
    "                \"log.logs\": \"programLogs\", \n",
    "                \"transaction.signature\": \"txSignature\", \n",
    "                \"transaction.success\": \"txSuccess\", \n",
    "                \"transaction.feePayer\": \"txSigner\",\n",
    "                \"data.base58\": \"instructionData\"\n",
    "            }\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_ix_batch(self):\n",
    "        \"\"\"\n",
    "        Batch the instruction retrieval. Save the shit Drop duplicates. \n",
    "\n",
    "        \"\"\"\n",
    "                \n",
    "        # Batch the days up nice and good so the graphql API calls don't bitch\n",
    "        dates_batched = pd.date_range(self.date_start, self.date_end, freq='7D')\n",
    "        dates_batched = [str(x.isoformat()) for x in dates_batched.append(pd.DatetimeIndex([self.date_end]))]\n",
    "        date_ranges = list(zip(dates_batched, dates_batched[1:]))\n",
    "        \n",
    "        ixs = []\n",
    "        \n",
    "        for date_range in date_ranges:\n",
    "            assert len(date_range)==2\n",
    "            data = self.get_ix(date_range[0], date_range[1])\n",
    "            ixs.append(data)\n",
    "            \n",
    "        df_ix = pd.concat(ixs, ignore_index=False)\n",
    "        df_ix[\"instructionType\"] = df_ix.instructionData.apply(lambda x: self.instruction_match(x))\n",
    "        \n",
    "        # Store df_ix before we write it to the DataFrame so we avoid getting xfers for every single ix\n",
    "        self.df_ix = df_ix.drop_duplicates()\n",
    "        print(datetime.now(), \"final instruction data size: \", df_ix.shape[0])\n",
    "\n",
    "        df_old = self.get_existing_df(self.ix_fname)\n",
    "        df = pd.concat([df_old, df_ix], ignore_index=True)\n",
    "        df.to_csv(self.ix_fname, index=False)\n",
    "        print(datetime.now(), \"wrote instruction data to csv...\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_batched_xfers(self, instructionType, fname):\n",
    "        \"\"\"\n",
    "        Get all transfers corresponding to a specific instructionType from Graphql query. \n",
    "        Batch these queries up b/c the string sizes are too large (curse GraphQL for not supporting joins)\n",
    "        \n",
    "        :instructionType: String corresponding to the instruction type of each query. \n",
    "        :fname: Name of where the old df is stored\n",
    "        \n",
    "        \"\"\"\n",
    "        # assert self.df_ix, \"Error: instructions get_ix_batch() must be called before xfers are scraped\"\n",
    "            \n",
    "        temp = self.df_ix.query(\"instructionType == '%s'\" % (instructionType))\n",
    "        \n",
    "        if temp.empty:\n",
    "            print(datetime.now(), \"instructionType was not found in the data... breaking\")\n",
    "            return\n",
    "        \n",
    "        tx_signatures = list(temp[\"txSignature\"].unique())\n",
    "        tx_strs = self.format_txs_for_query(tx_signatures)\n",
    "        print(datetime.now(), len(tx_strs), \"signature batches required...\")\n",
    "        xfers = []\n",
    "        \n",
    "        for i, tx_str in enumerate(tx_strs):\n",
    "            query = self.xfer_query % (tx_str)\n",
    "            result = self.run_query(query)\n",
    "            df = pd.json_normalize(result['data']['solana']['transfers'])\n",
    "            xfers.append(df)\n",
    "            print(datetime.now(), df.shape[0], \"transfers scraped in batch %d\" % i)\n",
    "\n",
    "        df_xfer = pd.concat(xfers, ignore_index=False)\n",
    "        df_xfer = df_xfer.rename(\n",
    "            columns={\n",
    "                \"block.timestamp.iso8601\": \"timestamp\", \n",
    "                \"instruction.action.name\": \"instructionAction\", \n",
    "                \"instruction.callPath\": \"instructionOrder\", \n",
    "                \"transaction.signer\": \"userAddress\", \n",
    "                \"transaction.signature\": \"txSignature\", \n",
    "                \"currency.symbol\": \"currencySymbol\", \n",
    "                \"currency.name\": \"currencyName\", \n",
    "                \"receiver.address\": \"receiverAddress\", \n",
    "                \"sender.address\": \"senderAddress\", \n",
    "                \"currency.decimals\": \"currencyDecimals\",\n",
    "                \"currency.address\": \"currencyAddress\", \n",
    "                \"sender.mintAccount\": \"senderTokenMint\"\n",
    "            }\n",
    "          )\n",
    "        \n",
    "        print(datetime.now(), df_xfer.shape[0], \"transfers retrieved\")\n",
    "\n",
    "        df_old = self.get_existing_df(fname)\n",
    "        df_final = df_old.append(df_xfer, ignore_index=True).sort_values(\"instructionOrder\")\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "\n",
    "    def parse_deposits(self):\n",
    "        instructionType = 'Deposit'\n",
    "        instructionAction = \"transfer\"\n",
    "        tx_merge_key = \"receiverAddress\"\n",
    "        meta_merge_key = \"vaultAuthority\"\n",
    "        out_file = self.deposit_fname\n",
    "        \n",
    "        self.parse_base(instructionType, instructionAction, tx_merge_key, meta_merge_key, out_file)\n",
    "        \n",
    "\n",
    "    def parse_withdrawal(self):\n",
    "        instructionType = 'Withdraw'\n",
    "        instructionAction = \"burn\"\n",
    "        tx_merge_key = \"currencyAddress\"\n",
    "        meta_merge_key = \"shareTokenMint\"\n",
    "        out_file = self.withdraw_fname\n",
    "        \n",
    "        self.parse_base(instructionType, instructionAction, tx_merge_key, meta_merge_key, out_file)\n",
    "        \n",
    "        \n",
    "    def parse_deposit_cancel(self):\n",
    "        instructionType = 'CancelPendingDeposit'\n",
    "        instructionAction = \"transfer\"\n",
    "        tx_merge_key = \"senderAddress\"\n",
    "        meta_merge_key = \"vaultAuthority\"\n",
    "        out_file = self.deposit_cxl_fname\n",
    "        \n",
    "        self.parse_base(instructionType, instructionAction, tx_merge_key, meta_merge_key, out_file)\n",
    "                \n",
    "        \n",
    "    def parse_withdrawal_cancel(self):\n",
    "        instructionType = 'CancelPendingWithdrawal'\n",
    "        instructionAction = \"mintTo\"\n",
    "        tx_merge_key = \"currencyAddress\"\n",
    "        meta_merge_key = \"shareTokenMint\"\n",
    "        out_file = self.withdraw_cxl_fname\n",
    "        \n",
    "        self.parse_base(instructionType, instructionAction, tx_merge_key, meta_merge_key, out_file)\n",
    "\n",
    "        \n",
    "    def parse_claim_withdrawal(self):\n",
    "        instructionType = 'ClaimPendingWithdrawal'\n",
    "        instructionAction = \"transfer\"\n",
    "        tx_merge_key = \"senderAddress\"\n",
    "        meta_merge_key = \"vaultAuthority\"\n",
    "        out_file = self.withdraw_claim_fname\n",
    "\n",
    "        self.parse_base(instructionType, instructionAction, tx_merge_key, meta_merge_key, out_file)\n",
    "\n",
    "        \n",
    "    def parse_base(self, instructionType, instructionAction, tx_merge_key, meta_merge_key, output_file):\n",
    "        \"\"\"\n",
    "        generalized method for parsing transfer data. \n",
    "        \n",
    "        1. Call get_batched_xfers()\n",
    "        2. for each unique txSignature, find the xfer matching to the last instance of the instructionAction\n",
    "        3. Join it to the friktion metadata based using tx_merge_key and meta_merge_key\n",
    "        4. Drop extraneous rows\n",
    "        5. Save the file to the output_file\n",
    "        \n",
    "        :instructionType: Type of instruction listed out in the instruction_match() method\n",
    "        :instructionAction: type of transfer we are matching towards\n",
    "        :tx_merge_key: what key in the xfer dataFrame do we want to merge on\n",
    "        :meta_merge_key: what key in the metadata dataFrame we want to merge on. \n",
    "        :output_file: as name suggests\n",
    "        \"\"\"\n",
    "        print(datetime.now(), \"Parsing transfers for instructionType: %s\" % instructionType)\n",
    "        df = self.get_batched_xfers(instructionType, output_file)\n",
    "        \n",
    "        # Get rid of confusing wSOL entries\n",
    "        df = df.query('currencyName != \"Wrapped SOL\"')\n",
    "\n",
    "        # The deposit is always the last mintTo instruction.\n",
    "        df = df.query('instructionAction==\"{}\"'.format(instructionAction)).groupby(\"txSignature\").last().reset_index()\n",
    "        \n",
    "        df = pd.merge(df, self.friktion_metadata, how='left',\n",
    "                      left_on=tx_merge_key, right_on=meta_merge_key, suffixes=('', '_drop'))\n",
    "        df.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)\n",
    "        \n",
    "        df.drop_duplicates().to_csv(output_file, index=False)\n",
    "        print(datetime.now(), \"{} data size: {}\".format(instructionType, df.shape[0]))  \n",
    "        \n",
    "        \n",
    "    def parse_all(self):\n",
    "        self.get_ix_batch()\n",
    "        self.parse_claim_withdrawal()\n",
    "        self.parse_deposit_cancel()\n",
    "        self.parse_withdrawal_cancel()\n",
    "        self.parse_deposits()\n",
    "        self.parse_withdrawal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3a4d7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = \"2022-03-15T00:00:00Z\"\n",
    "date_end = \"2022-03-18T00:00:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "85819620",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = MyPortfolio(date_start, date_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d1cf2981",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-15T00:00:00+00:00 2022-03-18T00:00:00+00:00\n",
      "2022-03-31 00:20:23.012834 retrieving instructions for 2022-03-15T00:00:00+00:00 to 2022-03-18T00:00:00+00:00\n",
      "2022-03-31 00:20:29.453841 963 instructions retrieved\n",
      "2022-03-31 00:20:29.458655 final instruction data size:  963\n",
      "2022-03-31 00:20:29.523217 wrote instruction data to csv...\n",
      "2022-03-31 00:20:29.523519 Parsing transfers for instructionType: ClaimPendingWithdrawal\n",
      "2022-03-31 00:20:29.524992 1 signature batches required...\n",
      "2022-03-31 00:20:31.975552 160 transfers scraped in batch 0\n",
      "2022-03-31 00:20:31.976718 160 transfers retrieved\n",
      "2022-03-31 00:20:31.994906 ClaimPendingWithdrawal data size: 56\n",
      "2022-03-31 00:20:31.995139 Parsing transfers for instructionType: CancelPendingDeposit\n",
      "2022-03-31 00:20:31.996858 1 signature batches required...\n",
      "2022-03-31 00:20:33.566696 65 transfers scraped in batch 0\n",
      "2022-03-31 00:20:33.571413 65 transfers retrieved\n",
      "2022-03-31 00:20:33.599703 CancelPendingDeposit data size: 25\n",
      "2022-03-31 00:20:33.599946 Parsing transfers for instructionType: CancelPendingWithdrawal\n",
      "2022-03-31 00:20:33.601729 1 signature batches required...\n",
      "2022-03-31 00:20:34.766961 14 transfers scraped in batch 0\n",
      "2022-03-31 00:20:34.770550 14 transfers retrieved\n",
      "2022-03-31 00:20:34.783515 CancelPendingWithdrawal data size: 14\n",
      "2022-03-31 00:20:34.783642 Parsing transfers for instructionType: Deposit\n",
      "2022-03-31 00:20:34.784985 6 signature batches required...\n",
      "2022-03-31 00:20:40.867533 298 transfers scraped in batch 0\n",
      "2022-03-31 00:20:47.134434 327 transfers scraped in batch 1\n",
      "2022-03-31 00:20:53.649139 343 transfers scraped in batch 2\n",
      "2022-03-31 00:21:02.879434 336 transfers scraped in batch 3\n",
      "2022-03-31 00:21:07.956670 282 transfers scraped in batch 4\n",
      "2022-03-31 00:21:11.351809 123 transfers scraped in batch 5\n",
      "2022-03-31 00:21:11.356173 1709 transfers retrieved\n",
      "2022-03-31 00:21:11.390787 Deposit data size: 417\n",
      "2022-03-31 00:21:11.391151 Parsing transfers for instructionType: Withdraw\n",
      "2022-03-31 00:21:11.392353 3 signature batches required...\n",
      "2022-03-31 00:21:21.709746 256 transfers scraped in batch 0\n",
      "2022-03-31 00:21:32.346875 340 transfers scraped in batch 1\n",
      "2022-03-31 00:21:36.230881 7 transfers scraped in batch 2\n",
      "2022-03-31 00:21:36.243983 603 transfers retrieved\n",
      "2022-03-31 00:21:36.260180 Withdraw data size: 153\n"
     ]
    }
   ],
   "source": [
    "x.parse_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21f582",
   "metadata": {},
   "source": [
    "# Data Fidelity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ce144c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_files():\n",
    "    import os\n",
    "    os.remove('*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7f0b8ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '*.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/lm3g0brx0dsf0ymt1ldc8xdr0000gp/T/ipykernel_36449/1739331823.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclear_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/t5/lm3g0brx0dsf0ymt1ldc8xdr0000gp/T/ipykernel_36449/3883891080.py\u001b[0m in \u001b[0;36mclear_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclear_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '*.csv'"
     ]
    }
   ],
   "source": [
    "clear_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c1ece2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ix = pd.read_csv(\"friktion_ix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4792a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deposits = pd.read_csv(\"friktion_deposit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c87142c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "withdrawals = pd.read_csv(\"friktion_withdraw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "508f254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = pd.read_csv(\"friktion_claim_withdrawal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f46cf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "withdrawals_cxl = pd.read_csv(\"friktion_withdraw_cancel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b2190433",
   "metadata": {},
   "outputs": [],
   "source": [
    "deposits_cxl = pd.read_csv(\"friktion_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de0a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
