{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python script to scrape Friktion User Data from Bitquery GraphQL API.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import requests\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "class MyPortfolio:\n",
    "    \"\"\"\n",
    "    Python ETL script for Friktion user portfolio data. Currently supported Instruction Names:\n",
    "        - Deposit\n",
    "        - CancelPendingDeposit\n",
    "        - Withdrawal\n",
    "        - CancelPendingWithdrawal\n",
    "        - ClaimPendingWithdrawal\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        date_start,\n",
    "        date_end,\n",
    "        ix_fname=\"./friktion_ix2.csv\",\n",
    "        deposit_fname=\"./friktion_deposit.csv\",\n",
    "        deposit_cxl_fname=\"./friktion_deposit_cxl.csv\",\n",
    "        withdraw_fname=\"./friktion_withdraw.csv\",\n",
    "        withdraw_cxl_fname=\"./friktion_withdraw_cancel.csv\",\n",
    "        withdraw_claim_fname=\"./friktion_claim_withdrawal.csv\",\n",
    "        endround_fname=\"./friktion_end_round4.csv\",\n",
    "        batch_size_days=3,\n",
    "        batch_size_xfers=40,\n",
    "        skip_ix_scrape=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :ix_fname:              output csv for instructions\n",
    "        :deposit_fname:         output csv for deposits\n",
    "        :deposit_cxl_fname:     output csv for deposit cancels\n",
    "        :withdraw_fname:        output csv for withdrawals\n",
    "        :withdraw_cxl_fname:    output csv for withdrawal cancels\n",
    "        :withdraw_claim_fname:  output csv for claiming pending withdrawal\n",
    "        :batch_size_days:       batch size in days for query to keep query < 10k symbols. Use bigger steps for larger data.\n",
    "        :batch_size_transfers:  batch size transactions for query to keep query < 8kb\n",
    "\n",
    "        \"\"\"\n",
    "        print(date_start, date_end)\n",
    "        self.volt_program = \"VoLT1mJz1sbnxwq5Fv2SXjdVDgPXrb9tJyC8WpMDkSp\"\n",
    "        self.date_start = date_start\n",
    "        self.date_end = date_end\n",
    "        self.ix_fname = ix_fname\n",
    "        self.deposit_fname = deposit_fname\n",
    "        self.deposit_cxl_fname = deposit_cxl_fname\n",
    "        self.withdraw_fname = withdraw_fname\n",
    "        self.withdraw_cxl_fname = withdraw_cxl_fname\n",
    "        self.withdraw_claim_fname = withdraw_claim_fname\n",
    "        self.end_round_fname = endround_fname\n",
    "        self.batch_size_days = batch_size_days\n",
    "        self.batch_size_xfers = batch_size_xfers\n",
    "        self.skip_ix_scrape = skip_ix_scrape\n",
    "        \n",
    "        self.df_ix = pd.read_csv(ix_fname) if skip_ix_scrape else [] \n",
    "        print(\"print_ix\", self.df_ix)\n",
    "        self.friktion_metadata = self.get_friktion_snapshot()\n",
    "\n",
    "    ########################################################################################################\n",
    "    ####################################          Queries             ######################################\n",
    "    ########################################################################################################\n",
    "\n",
    "    @property\n",
    "    def ix_query(self):\n",
    "        return \"\"\"\n",
    "            query MyQuery {\n",
    "              solana {\n",
    "                instructions(\n",
    "                  time: {between: [\"%s\", \"%s\"]}\n",
    "                  success: {is: true}\n",
    "                  programId: {is: \"VoLT1mJz1sbnxwq5Fv2SXjdVDgPXrb9tJyC8WpMDkSp\"}\n",
    "                  options: {limit: 8700}\n",
    "                ) {\n",
    "                  block {\n",
    "                    timestamp {\n",
    "                      iso8601\n",
    "                    }\n",
    "                  }\n",
    "                  transaction {\n",
    "                    signature\n",
    "                    feePayer\n",
    "                  }\n",
    "                  data {\n",
    "                    base58\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    def xfer_query(self):\n",
    "        return \"\"\"\n",
    "            query MyQuery {\n",
    "              solana(network: solana) {\n",
    "                transfers(\n",
    "                  signature: {in: [%s]}\n",
    "                  options: {limit: 2000}\n",
    "                ) {\n",
    "                  instruction {\n",
    "                    action {\n",
    "                      name\n",
    "                    }\n",
    "                    callPath\n",
    "                  }\n",
    "                  amount(success: {is: true})\n",
    "                  transaction {\n",
    "                    signer\n",
    "                    signature\n",
    "                  }\n",
    "                  block {\n",
    "                    timestamp {\n",
    "                      iso8601\n",
    "                    }\n",
    "                  }\n",
    "                  currency {\n",
    "                    name\n",
    "                    address\n",
    "                  }\n",
    "                  sender {\n",
    "                    address\n",
    "                    mintAccount\n",
    "                  }\n",
    "                  receiver {\n",
    "                    address\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "    ########################################################################################################\n",
    "    ################################          Helper Functions             #################################\n",
    "    ########################################################################################################\n",
    "\n",
    "    # TODO: Add retry logic to this in case of hangups.\n",
    "    @staticmethod\n",
    "    def run_query(query, retries=10):\n",
    "        \"\"\"\n",
    "        Query graphQL API.\n",
    "\n",
    "        If timeerror\n",
    "        \"\"\"\n",
    "        headers = {\"X-API-KEY\": \"BQYCaXaMZlqZrPCSQVsiJrKtxKRVcSe4\"}\n",
    "\n",
    "        retries_counter = 0\n",
    "        try:\n",
    "            request = requests.post(\n",
    "                \"https://graphql.bitquery.io/\", json={\"query\": query}, headers=headers\n",
    "            )\n",
    "            result = request.json()\n",
    "            # print(dir(request.content))\n",
    "            # Make sure that there is no error message\n",
    "            # assert not request.content.errors\n",
    "            assert \"errors\" not in result\n",
    "        except:\n",
    "            while (\n",
    "                (request.status_code != 200\n",
    "                or \"errors\" in result)\n",
    "                and retries_counter < 10\n",
    "            ):\n",
    "                print(datetime.now(), f\"Retry number {retries_counter}\")\n",
    "                if \"errors\" in result:\n",
    "                    print(result[\"errors\"])\n",
    "                print(datetime.now(), f\"Query failed for reason: {request.reason}. sleeping for {150*retries_counter} seconds and retrying...\")\n",
    "                time.sleep(150*retries_counter)\n",
    "                request = requests.post(\n",
    "                    \"https://graphql.bitquery.io/\",\n",
    "                    json={\"query\": query},\n",
    "                    headers=headers,\n",
    "                )\n",
    "                retries_counter += 1\n",
    "            if retries_counter >= retries:\n",
    "                raise Exception(\n",
    "                    \"Query failed after {} retries and return code is {}.{}\".format(\n",
    "                        retries_counter, request.status_code, query\n",
    "                    )\n",
    "                )\n",
    "        return request.json()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_iterable(iterable, n=1):\n",
    "        \"\"\"\n",
    "        Takes in an iterable and returns an iterable of iterables with len==x\n",
    "        \"\"\"\n",
    "        idxs = []\n",
    "        l = len(iterable)\n",
    "        for idx in range(0, l, n):\n",
    "            idxs.append(iterable[idx : min(idx + n, l)])\n",
    "        return idxs\n",
    "\n",
    "    def format_txs_for_query(self, tx_signatures):\n",
    "        \"\"\"\n",
    "        Batches a list of transactions into a list of string formatted transactions for querying.\n",
    "        Each of these strings contain (n=self.batch_size_xfers) unique transaction IDs.\n",
    "        \"\"\"\n",
    "        batched_signatures = self.batch_iterable(tx_signatures, self.batch_size_xfers)\n",
    "\n",
    "        def format_txs(x):\n",
    "            return str(x)[1:-1].replace(\"'\", '\"').replace(\"\\n\", \"\")\n",
    "\n",
    "        tx_strs = list(map(format_txs, batched_signatures))\n",
    "\n",
    "        return tx_strs\n",
    "\n",
    "    def get_existing_df(self, fname):\n",
    "        \"\"\"\n",
    "        Create output file if doesn't exist and returns a DataFrame\n",
    "        \"\"\"\n",
    "        if fname and exists(fname):\n",
    "            return pd.read_csv(fname)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @staticmethod\n",
    "    def instruction_match(instructionData):\n",
    "        \"\"\"\n",
    "        Match 8-bit instruction identifier with the corresonding instructionType\n",
    "        \"\"\"\n",
    "        if not instructionData or len(instructionData) < 8:\n",
    "            return False\n",
    "\n",
    "        instructionDescriptor = instructionData[:8]\n",
    "\n",
    "        if instructionDescriptor == \"A4eeE44X\" or instructionDescriptor == \"7h4w9t5c\" or instructionDescriptor == \"K84hiiqe\":\n",
    "            print(\"EndRound: \", instructionDescriptor)\n",
    "            return \"EndRound\"\n",
    "\n",
    "    def get_friktion_snapshot(self):\n",
    "        \"\"\"\n",
    "        Load Friktion Metadata for Volt/Symbol Mapping to join to normal data\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.DataFrame(\n",
    "                dict(\n",
    "                    json.loads(\n",
    "                        requests.get(\n",
    "                            \"https://friktion-labs.github.io/mainnet-tvl-snapshots/friktionSnapshot.json\"\n",
    "                        ).content\n",
    "                    )\n",
    "                )[\"allMainnetVolts\"]\n",
    "            )[\n",
    "                [\n",
    "                    \"globalId\",\n",
    "                    \"vaultAuthority\",\n",
    "                    \"shareTokenMint\",\n",
    "                    \"depositTokenSymbol\",\n",
    "                    \"depositTokenCoingeckoId\",\n",
    "                ]\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(datetime.now(), \"Snapshot Data Invalid\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    ########################################################################################################\n",
    "    ################################          Data Retrieval             ###################################\n",
    "    ########################################################################################################\n",
    "\n",
    "    def get_ix(self, date_start, date_end):\n",
    "        \"\"\"\n",
    "        Runs graphql instruction query for one date range.\n",
    "        \"\"\"\n",
    "        print(date_start, date_end)\n",
    "        query = self.ix_query % (date_start, date_end)\n",
    "        print(\n",
    "            datetime.now(),\n",
    "            \"retrieving instructions for {} to {}\".format(date_start, date_end),\n",
    "        )\n",
    "        result = self.run_query(query)\n",
    "\n",
    "        # convert GraphQL json to pandas dataframe\n",
    "        try:\n",
    "            df = pd.json_normalize(result[\"data\"][\"solana\"][\"instructions\"])\n",
    "        except:\n",
    "            print(result)\n",
    "            traceback.print_exc()\n",
    "            raise Exception(datetime.now(), \"Empty Results... Try Again\")\n",
    "\n",
    "        print(datetime.now(), df.shape[0], \"instructions retrieved\")\n",
    "\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                \"block.timestamp.iso8601\": \"timestamp\",\n",
    "                \"log.consumed\": \"computeUnits\",\n",
    "                \"transaction.signature\": \"txSignature\",\n",
    "                \"transaction.feePayer\": \"txSigner\",\n",
    "                \"data.base58\": \"instructionData\",\n",
    "            }\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def get_ix_batch(self):\n",
    "        \"\"\"\n",
    "        Batch the instruction retrieval. Save the shit Drop duplicates.\n",
    "\n",
    "        \"\"\"\n",
    "        # Batch the days up nice and good so the graphql API calls don't bitch\n",
    "        dates_batched = pd.date_range(self.date_start, self.date_end, freq=f\"{self.batch_size_days}D\")\n",
    "        dates_batched = [\n",
    "            str(x.isoformat())\n",
    "            for x in dates_batched.append(pd.DatetimeIndex([self.date_end]))\n",
    "        ]\n",
    "        date_ranges = list(zip(dates_batched, dates_batched[1:]))\n",
    "        ixs = []\n",
    "        for date_range in date_ranges:\n",
    "            assert len(date_range) == 2\n",
    "            data = self.get_ix(date_range[0], date_range[1])\n",
    "            ixs.append(data)\n",
    "\n",
    "        df_ix = pd.concat(ixs, ignore_index=False)\n",
    "        df_ix[\"instructionType\"] = df_ix.instructionData.apply(\n",
    "            lambda x: self.instruction_match(x)\n",
    "        )\n",
    "\n",
    "        # Store df_ix before we write it to the DataFrame so we avoid getting xfers for every single ix \n",
    "        self.df_ix = df_ix.drop_duplicates([\"txSignature\", \"instructionType\"]).reset_index(drop=True)\n",
    "        print(datetime.now(), \"final instruction data size: \", df_ix.shape[0])\n",
    "\n",
    "        # Save new data to file\n",
    "        df_old = self.get_existing_df(self.ix_fname)\n",
    "        df = df_ix.append(df_old).reset_index(drop=True)\n",
    "        df = df.drop_duplicates([\"txSignature\", \"instructionType\"])\n",
    "        df.to_csv(self.ix_fname, index=False)\n",
    "        print(datetime.now(), \"wrote instruction data to csv...\")\n",
    "\n",
    "    def get_batched_xfers(self, instructionType, fname):\n",
    "        \"\"\"\n",
    "        Get all transfers corresponding to a specific instructionType from Graphql query.\n",
    "        Batch these queries up b/c the string size is\n",
    "        too large (curse GraphQL for not supporting joins)\n",
    "\n",
    "        :instructionType: String corresponding to the instruction type of each query.\n",
    "        :fname: Name of where the old df is stored\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        temp = self.df_ix.query(f\"instructionType == '{instructionType}'\")\n",
    "\n",
    "        if temp.empty:\n",
    "            print(\n",
    "                datetime.now(), \"instructionType was not found in the data... breaking\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        tx_signatures = list(temp[\"txSignature\"].unique())\n",
    "        tx_strs = self.format_txs_for_query(tx_signatures)\n",
    "        print(datetime.now(), len(tx_strs), \"signature batches required...\")\n",
    "        xfers = []\n",
    "\n",
    "        for i, tx_str in enumerate(tx_strs):\n",
    "            query = self.xfer_query % (tx_str)\n",
    "            result = self.run_query(query)\n",
    "            # TODO: Clean up this duplicated logic\n",
    "            try:\n",
    "                df = pd.json_normalize(result[\"data\"][\"solana\"][\"transfers\"])\n",
    "            except:\n",
    "                print(result)\n",
    "                traceback.print_exc()\n",
    "                raise Exception(\"Empty Results... Try Again\")\n",
    "            xfers.append(df)\n",
    "            print(datetime.now(), df.shape[0], \"transfers scraped in batch %d\" % i)\n",
    "\n",
    "        df_xfer = pd.concat(xfers, ignore_index=False)\n",
    "        print(\"HELLO\", df_xfer.loc[df_xfer[\"transaction.signature\"] == \"5b9HR69oQ61KBC5V5rsWr4kXh2PK3gEGhcqMvWyANvxvM9U2KPrxEXooPAoXo8HbeYndgZMJYcV4AJE38K33P5xy\"])\n",
    "        df_xfer = df_xfer.rename(\n",
    "            columns={\n",
    "                \"block.timestamp.iso8601\": \"timestamp\",\n",
    "                \"instruction.action.name\": \"instructionAction\",\n",
    "                \"instruction.callPath\": \"instructionOrder\",\n",
    "                \"transaction.signer\": \"userAddress\",\n",
    "                \"transaction.signature\": \"txSignature\",\n",
    "                \"currency.name\": \"currencyName\",\n",
    "                \"receiver.address\": \"receiverAddress\",\n",
    "                \"sender.address\": \"senderAddress\",\n",
    "                \"currency.decimals\": \"currencyDecimals\",\n",
    "                \"currency.address\": \"currencyAddress\",\n",
    "                \"sender.mintAccount\": \"senderTokenMint\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(datetime.now(), df_xfer.shape[0], \"transfers retrieved\")\n",
    "\n",
    "        return df_xfer\n",
    "        \n",
    "    def parse_endRound(self):\n",
    "        instructionType = \"EndRound\"\n",
    "        instructionAction = \"transfer\"\n",
    "        tx_merge_key = \"currencyAddress\"\n",
    "        meta_merge_key = \"shareTokenMint\"\n",
    "        out_file = self.end_round_fname\n",
    "\n",
    "        self.parse_base(\n",
    "            instructionType, instructionAction, tx_merge_key, meta_merge_key, out_file\n",
    "        )\n",
    "        \n",
    "\n",
    "    def parse_base(\n",
    "        self,\n",
    "        instructionType,\n",
    "        instructionAction,\n",
    "        tx_merge_key,\n",
    "        meta_merge_key,\n",
    "        output_file,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        generalized method for parsing transfer data.\n",
    "\n",
    "        1. Call get_batched_xfers()\n",
    "        2. for each unique txSignature, find the xfer matching to the last instance of the instructionAction\n",
    "        3. For SOL CC vaults, need an extra step and query for instructionAccount to find vaultAuthority.\n",
    "        3. Join it to the friktion metadata based using tx_merge_key and meta_merge_key\n",
    "        4. Drop extraneous rows\n",
    "        5. Save the file to the output_file\n",
    "\n",
    "        :instructionType: Type of instruction listed out in the instruction_match() method\n",
    "        :instructionAction: type of transfer we are matching towards\n",
    "        :tx_merge_key: what key in the xfer dataFrame do we want to merge on\n",
    "        :output_file: as name suggests\n",
    "        \"\"\"\n",
    "        print(\n",
    "            datetime.now(),\n",
    "            \"Parsing transfers for instructionType: %s\" % instructionType,\n",
    "        )\n",
    "        df = self.get_batched_xfers(instructionType, output_file)\n",
    "#         Target only wrapped SOL entries for SOL vaults\n",
    "        df = df.query('currencyName != \"Solana\"')\n",
    "#         )\n",
    "        shareTokendf = df.query(\"instructionAction=='mintTo'\").copy()\n",
    "        backup = df.query(\"instructionOrder=='0-0'\").copy()\n",
    "        # Backup-entropy\n",
    "        backup_entropy = df.query(\"instructionOrder=='0-0-0'\").copy()\n",
    "\n",
    "    \n",
    "        def find_matching_call_path(x, entropy=False):\n",
    "            [path1, path2] = x.split(\"-\")\n",
    "            newCallPath = path1+\"-\"+str(int(path2)-1)\n",
    "            if entropy:\n",
    "                newCallPath=x\n",
    "            return newCallPath\n",
    "        \n",
    "        shareTokendf[\"instructionOrderPair\"] = shareTokendf.instructionOrder.apply(find_matching_call_path)\n",
    "        shareTokendf[\"instructionOrderPair_Entropy\"] = shareTokendf.instructionOrder.apply(find_matching_call_path, entropy=True)\n",
    "        \n",
    "        df_backup = df.copy()\n",
    "        df = pd.merge(shareTokendf, df, left_on=[\"txSignature\", \"instructionOrderPair\"], \n",
    "                      right_on=[\"txSignature\", \"instructionOrder\"], suffixes=(\"\", \"_x\"))\n",
    "        \n",
    "        df = pd.merge(backup, df, left_on=[\"txSignature\"], \n",
    "                      right_on=[\"txSignature\"], suffixes=(\"_y\", \"\"), how='right')\n",
    "\n",
    "        \n",
    "        df[\"shareTokenPrice\"] = df.amount_x/df.amount\n",
    "        df[\"backupValue\"] = df.amount_y\n",
    "        # Join tables and get rid of extraneous columns from metadata.\n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            self.friktion_metadata,\n",
    "            how=\"left\",\n",
    "            left_on=tx_merge_key,\n",
    "            right_on=meta_merge_key,\n",
    "            suffixes=(\"\", \"_drop\"),\n",
    "        )\n",
    "        \n",
    "        df.drop([col for col in df.columns if \"drop\" in col], axis=1, inplace=True)\n",
    "\n",
    "        # Tag the row with the instructionType\n",
    "        df[\"userAction\"] = instructionType\n",
    "\n",
    "        df.drop_duplicates().to_csv(output_file, index=False)\n",
    "        print(datetime.now(), \"{} data size: {}\".format(instructionType, df.shape[0]))\n",
    "\n",
    "    def parse_all(self):\n",
    "        if not self.skip_ix_scrape:\n",
    "            self.get_ix_batch()\n",
    "        self.parse_endRound()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
